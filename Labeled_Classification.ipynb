{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5086a3ed-1067-4a32-9edc-659b5ab440e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from astropy.table import Table\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ade83f-d7bb-44e3-9155-f667d443e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reading the data\n",
    "# dft = Table.read('/home/samanehjavadinia/Co-op/Data/ngvs_matched.fits')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d0c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the paths to your CSV files\n",
    "# cleaned_data_path = \"/home/samanehjavadinia/Co-op/Data/cleaned_dataset.csv\"\n",
    "# unlabeled_data_path = '/home/samanehjavadinia/Co-op/Data/unlabeled_dataset.csv'\n",
    "\n",
    "# # Define the chunk size\n",
    "# chunk_size = 10000  # Adjust as needed\n",
    "\n",
    "# # Function to read and concatenate chunks\n",
    "# def read_large_csv(file_path, chunk_size):\n",
    "#     chunk_iter = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "#     df_list = [chunk for chunk in chunk_iter]  # List of dataframes\n",
    "#     return pd.concat(df_list, ignore_index=True)  # Concatenate all chunks\n",
    "\n",
    "# # Read the cleaned dataset\n",
    "# print(\"Reading cleaned dataset...\")\n",
    "# df = read_large_csv(cleaned_data_path, chunk_size)\n",
    "# print(\"Cleaned dataset loaded.\")\n",
    "\n",
    "# # # Read the unlabeled dataset\n",
    "# # print(\"Reading unlabeled dataset...\")\n",
    "# # unlabeled_data = read_large_csv(unlabeled_data_path, chunk_size)\n",
    "# # print(\"Unlabeled dataset loaded.\")\n",
    "\n",
    "# # Now df_cleaned and df_unlabeled are dataframes and you can work on them\n",
    "# print(\"Cleaned DataFrame shape:\", df.shape)\n",
    "# # print(\"Unlabeled DataFrame shape:\", unlabeled_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ed6ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples from unlabeled dataset...\n",
      "Sample from unlabeled dataset loaded.\n",
      "Sampled Unlabeled DataFrame shape: (10000, 84)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the paths to your CSV files\n",
    "cleaned_data_path = \"/home/samanehjavadinia/Co-op/Data/cleaned_dataset.csv\"\n",
    "unlabeled_data_path = '/home/samanehjavadinia/Co-op/Data/unlabeled_dataset.csv'\n",
    "\n",
    "# Number of samples to read\n",
    "num_samples = 10000  # Adjust as needed\n",
    "\n",
    "# Function to read only a few samples from a large CSV file\n",
    "def read_sample_csv(file_path, num_samples):\n",
    "    return pd.read_csv(file_path, nrows=num_samples)\n",
    "\n",
    "df = pd.read_csv(cleaned_data_path)\n",
    "\n",
    "# Read samples from the unlabeled dataset\n",
    "print(\"Reading samples from unlabeled dataset...\")\n",
    "unlabeled_data = read_sample_csv(unlabeled_data_path, num_samples)\n",
    "print(\"Sample from unlabeled dataset loaded.\")\n",
    "\n",
    "# Now df_sample and unlabeled_sample are dataframes containing the samples\n",
    "print(\"Sampled Unlabeled DataFrame shape:\", unlabeled_data.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0055c65-83e0-493e-9eac-288af6bb494e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48166, 84)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e9f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b21d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_with_99(df):\n",
    "    \"\"\"\n",
    "    Drops rows from the DataFrame where any of the features have a value of 99.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with rows containing the value 99 dropped.\n",
    "    \"\"\"\n",
    "    return  df[~(df == 99).any(axis=1)]\n",
    "\n",
    "\n",
    "dft_cleaned = drop_rows_with_99(df)\n",
    "print(dft_cleaned.shape)\n",
    "\n",
    "unlabeled_data_cleaned = drop_rows_with_99(unlabeled_data)\n",
    "print(unlabeled_data_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4200bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = dft_cleaned.copy()\n",
    "unlabeled_data = unlabeled_data_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507eb478",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = dft.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7568a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft['Z_MAGERR_ISO'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8889c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in dft.columns:\n",
    "#     plt.figure()\n",
    "#     dft[column].hist()\n",
    "#     plt.title(f'Histogram of {column} in Cleaned Dataset')\n",
    "#     plt.xlabel(column)\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562ea12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to omit rows where umag is more than 30 or err is more than 0.5\n",
    "filtered_dft = dft[(dft['U_MAG_ISO'] <= 30) & (dft['U_MAGERR_ISO'] <= 0.5) & \n",
    "                   (dft['G_MAGERR_ISO'] <= 0.5) & (dft['R_MAGERR_ISO']<= 0.5) & \n",
    "                   (dft['I_MAGERR_ISO']<= 0.5) & (dft['Z_MAGERR_ISO']<= 0.5)]\n",
    "filtered_unlabeled_data = unlabeled_data[(unlabeled_data['U_MAG_ISO'] <= 30) \n",
    "                        & (unlabeled_data['U_MAGERR_ISO'] <= 0.5) & (unlabeled_data['G_MAGERR_ISO'] <= 0.5) &\n",
    "                (unlabeled_data['R_MAGERR_ISO']<= 0.5) & (unlabeled_data['I_MAGERR_ISO']<= 0.5)& (unlabeled_data['Z_MAGERR_ISO']<= 0.5)]\n",
    "\n",
    "print(filtered_dft.shape)\n",
    "print(filtered_unlabeled_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7695c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = filtered_dft.copy()\n",
    "unlabeled_data = filtered_unlabeled_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dft.shape)\n",
    "print(unlabeled_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f293a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the value counts of columns with fewer than 5 unique values\n",
    "def print_columns_with_fewer_than_5_unique_values(df):\n",
    "    for column in df.columns:\n",
    "        unique_values_count = df[column].nunique()\n",
    "        if unique_values_count < 5:\n",
    "            print(f\"Value counts for column '{column}' (unique values: {unique_values_count}):\")\n",
    "            print(df[column].value_counts())\n",
    "            print(\"\\n\")\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "print_columns_with_fewer_than_5_unique_values(dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987c0252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "print_columns_with_fewer_than_5_unique_values(unlabeled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6acba9",
   "metadata": {},
   "source": [
    "Feature Engineering Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7997ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_feature_differences_with_errors(df, feature_groups, error_groups, proxy_feature):\n",
    "    \"\"\"\n",
    "    Create all combinations of differences for each group of features and \n",
    "    the square root of the sum of squared errors for each combination.\n",
    "    \n",
    "    :param df: pandas DataFrame containing the data\n",
    "    :param feature_groups: List of lists, where each sublist contains feature names to create differences from\n",
    "    :param error_groups: List of lists, where each sublist contains error feature names corresponding to the feature groups\n",
    "    :return: DataFrame with new difference and error features and original features dropped\n",
    "    \"\"\"\n",
    "    for group, error_group in zip(feature_groups, error_groups):\n",
    "        print(group)\n",
    "        print(error_group)\n",
    "        # Create combinations of differences for each group\n",
    "        for i in range(len(group)):\n",
    "            for j in range(i + 1, len(group)):\n",
    "                diff_feature_name = f'{group[i]}_minus_{group[j]}'\n",
    "                error_feature_name = f'{error_group[i]}_minus_{error_group[j]}'\n",
    "                \n",
    "                df[diff_feature_name] = df[group[i]] - df[group[j]]\n",
    "                df[error_feature_name] = np.sqrt(df[error_group[i]]**2 + df[error_group[j]]**2)\n",
    "\n",
    "        # Drop the original features of the current group except the proxy feature\n",
    "        # print(group)\n",
    "        # Create a list of features to drop (all except proxy_feature)\n",
    "        features_to_drop = [feature for feature in group if feature != proxy_feature]\n",
    "        df.drop(columns=features_to_drop, inplace=True)\n",
    "        df.drop(columns=error_group, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Define the proxy feature to retain\n",
    "proxy_feature = 'G_MAG_ISO'\n",
    "\n",
    "# Define the feature groups and corresponding error groups\n",
    "feature_groups = [\n",
    "    ['U_MAG_ISO', 'G_MAG_ISO', 'R_MAG_ISO',\n",
    "       'I_MAG_ISO', 'Z_MAG_ISO']\n",
    "]\n",
    "\n",
    "error_groups = [\n",
    "    ['U_MAGERR_ISO', 'G_MAGERR_ISO',\n",
    "       'R_MAGERR_ISO', 'I_MAGERR_ISO', 'Z_MAGERR_ISO']\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply the function\n",
    "dft = create_feature_differences_with_errors(dft, feature_groups, error_groups, proxy_feature)\n",
    "unlabeled_data = create_feature_differences_with_errors(unlabeled_data, feature_groups, error_groups, proxy_feature)\n",
    "print(dft.columns)\n",
    "print(unlabeled_data.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b2c208-7ad9-4440-b1db-e76944c90404",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "dft.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c357d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft['main_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece29a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display the pie chart of the new column\n",
    "category_counts = dft['main_type'].value_counts()\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "category_counts.plot(kind='bar')\n",
    "plt.title('Bar Chart of Category Distribution')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each category\n",
    "category_counts = dft['main_type'].value_counts()\n",
    "\n",
    "# Specify the categorical column\n",
    "category_column = 'main_type'\n",
    "\n",
    "# Define the threshold for rare categories\n",
    "threshold = 300  # Categories with counts less than or equal to this will be grouped into \"Other\"\n",
    "\n",
    "# Count the occurrences of each category\n",
    "category_counts = dft[category_column].value_counts()\n",
    "\n",
    "# Identify the rare categories\n",
    "rare_categories = category_counts[category_counts <= threshold].index\n",
    "\n",
    "# Create a new column with grouped categories\n",
    "dft['grouped_category'] = dft[category_column].apply(lambda x: 'Other' if x in rare_categories else x)\n",
    "\n",
    "# Print the number of different categories in the new column\n",
    "num_categories = dft['grouped_category'].nunique()\n",
    "print(f'The number of different categories in the new grouped column: {num_categories}')\n",
    "\n",
    "\n",
    "\n",
    "# Count the occurrences of each category\n",
    "category_counts = dft['grouped_category'].value_counts()\n",
    "\n",
    "\n",
    "# Create and display the pie chart of the new column\n",
    "category_counts = dft['grouped_category'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Pie Chart of Grouped Categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2e3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "category_counts.plot(kind='bar')\n",
    "plt.title('Bar Chart of Category Distribution')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f2516",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft['grouped_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = dft.drop(columns='main_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2970008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique types in the 'grouped_category' column\n",
    "unique_types = dft['grouped_category'].apply(type).unique()\n",
    "print(\"Different types in 'grouped_category' column:\", unique_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac9318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all values in 'grouped_category' are strings\n",
    "dft['grouped_category'] = dft['grouped_category'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37af98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dft.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['main_type_encoded'] = label_encoder.fit_transform(df['grouped_category'])\n",
    "\n",
    "# Save the encoder to disk\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d6053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9cc1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Calculate the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Get the correlations of the features with the target\n",
    "correlation_with_target = correlation_matrix['main_type_encoded'].drop('main_type_encoded')\n",
    "\n",
    "\n",
    "# Sort correlations\n",
    "sorted_correlations = correlation_with_target.sort_values()\n",
    "\n",
    "# Plot the correlations\n",
    "plt.figure(figsize=(50, 50))\n",
    "sns.barplot(x=sorted_correlations.values, y=sorted_correlations.index, palette=\"viridis\")\n",
    "plt.title('Feature Correlations with Target')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec49d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0057aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.plotting import scatter_matrix\n",
    "\n",
    "# attributes = [ 'iC', 'iCerr', 'ebv', 'background_g', 'ug0', 'gi0', 'iz0', 'p_star',\n",
    "#        'p_gc', 'p_galx', 'umag_minus_gmag', 'umag_minus_rmag',\n",
    "#        'umag_minus_imag', 'umag_minus_zmag', 'gmag_minus_rmag',\n",
    "#        'gmag_minus_imag', 'gmag_minus_zmag', 'rmag_minus_imag',\n",
    "#        'rmag_minus_zmag', 'imag_minus_zmag', 'uerr_minus_gerr',\n",
    "#        'uerr_minus_rerr', 'uerr_minus_ierr', 'uerr_minus_zerr',\n",
    "#        'gerr_minus_rerr', 'gerr_minus_ierr', 'gerr_minus_zerr',\n",
    "#        'rerr_minus_ierr', 'rerr_minus_zerr', 'ierr_minus_zerr',\n",
    "#        'grouped_category', 'main_type_encoded']\n",
    "# scatter_matrix(df[attributes], figsize=(40, 40))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf9b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[ 'grouped_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb577f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data['main_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb095934",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data = unlabeled_data.drop(columns=[ 'main_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e0f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae433762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train and test datasets to CSV files\n",
    "df.to_csv('/home/samanehjavadinia/Co-op/Data/CFHT_modified_dataset.csv', index=False)\n",
    "unlabeled_data.to_csv('/home/samanehjavadinia/Co-op/Data/CFHT_unlabeled_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2cd78",
   "metadata": {},
   "source": [
    "#############LOading\n",
    "###########\n",
    "#######\n",
    "#########\n",
    "#########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944eca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from astropy.table import Table\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858647c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to load the data later, you can use the following code\n",
    "df = pd.read_csv('/home/samanehjavadinia/Co-op/Data/modified_dataset.csv')\n",
    "unlabeled_data = pd.read_csv('/home/samanehjavadinia/Co-op/Data/unlabeled_data.csv')\n",
    "# Load the encoder from disk\n",
    "label_encoder = joblib.load('label_encoder.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c86700",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e898da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the labels in the labeled dataset\n",
    "decoded_labels = label_encoder.inverse_transform(df['main_type_encoded'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448885f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(decoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506746ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count occurrences of \"GLCL\" and \"Other\"\n",
    "glcl_count = np.sum(decoded_labels == \"GLCL\")\n",
    "other_count = np.sum(decoded_labels == \"Other\")\n",
    "GLCL_CANDIDATE_count = np.sum(decoded_labels == 'GLCL?_CANDIDATE')\n",
    "total_count = len(decoded_labels)\n",
    "\n",
    "# Calculate percentage of \"GLCL\"\n",
    "glcl_percentage = (glcl_count / total_count) * 100\n",
    "\n",
    "print(f\"Number of 'GLCL' labels in Labeled Dataset: {glcl_count}\")\n",
    "print(f\"Number of 'Other' labels in Labeled Dataset: {other_count}\")\n",
    "print(f\"Number of 'GLCL_CANDIDATE' labels in Labeled Dataset: {GLCL_CANDIDATE_count}\")\n",
    "print(f\"Total number of labels in Labeled Dataset: {total_count}\")\n",
    "\n",
    "print(f\"Percentage of 'GLCL' labels in Labeled Dataset: {glcl_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c8975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary mapping for visualization\n",
    "binary_to_original_mapping = {\n",
    "    1: \"GLCL\",\n",
    "    0: 'Other'\n",
    "}\n",
    "\n",
    "# Map the decoded labels to binary form\n",
    "binary_labels = np.where(decoded_labels == \"GLCL\", 1, 0)\n",
    "\n",
    "# Decode binary labels to their original form using the mapping\n",
    "decoded_binary_labels = np.array([binary_to_original_mapping[label] for label in binary_labels])\n",
    "\n",
    "# Count occurrences of each label\n",
    "glcl_count = np.sum(decoded_binary_labels == \"GLCL\")\n",
    "other_count = np.sum(decoded_binary_labels == \"Other\")\n",
    "glcl_candidate_count = np.sum(decoded_labels == 'GLCL_CANDIDATE')  # Direct count from the original labels\n",
    "total_count = len(decoded_labels)\n",
    "\n",
    "# Calculate percentage of \"GLCL\"\n",
    "glcl_percentage = (glcl_count / total_count) * 100\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Number of 'GLCL' labels in Labeled Dataset: {glcl_count}\")\n",
    "print(f\"Number of 'Other' labels in Labeled Dataset: {other_count}\")\n",
    "print(f\"Number of 'GLCL_CANDIDATE' labels in Labeled Dataset: {glcl_candidate_count}\")\n",
    "print(f\"Total number of labels in Labeled Dataset: {total_count}\")\n",
    "print(f\"Percentage of 'GLCL' labels in Labeled Dataset: {glcl_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c70810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the original indices for the remaining rows\n",
    "original_indices = df['original_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e93c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1be0b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_indices.loc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b20fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ea31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['original_index']==2144155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98808207",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dfcfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_angdist = df.copy()\n",
    "df = df.drop(columns = 'angDist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3443838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['main_type_encoded'], random_state=42)\n",
    "\n",
    "# Further split the training data into a training set and a calibration set\n",
    "train_df, calib_df = train_test_split(train_df, test_size=0.2, stratify=train_df['main_type_encoded'], random_state=42)\n",
    "\n",
    "# Separate X and y for training and testing\n",
    "X_train = train_df.drop(columns=['main_type_encoded', 'original_index'])\n",
    "y_train = train_df['main_type_encoded']\n",
    "X_test = test_df.drop(columns=['main_type_encoded', 'original_index'])\n",
    "y_test = test_df['main_type_encoded']\n",
    "X_calib = calib_df.drop(columns=['main_type_encoded', 'original_index'])\n",
    "y_calib = calib_df['main_type_encoded']\n",
    "\n",
    "# Save the original indices for reference\n",
    "train_indices = train_df['original_index']\n",
    "test_indices = test_df['original_index']\n",
    "calib_indices = calib_df['original_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4a1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36892684",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices.loc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf54ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c7ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "def scale_numeric_columns(train_df, test_df, calib_df):\n",
    "    # Identify non-numeric and numeric columns\n",
    "    non_numeric_columns = train_df.select_dtypes(include=['object']).columns\n",
    "    numeric_columns = train_df.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "    # Separate non-numeric and numeric columns\n",
    "    train_non_numeric = train_df[non_numeric_columns]\n",
    "    train_numeric = train_df[numeric_columns]\n",
    "\n",
    "    test_non_numeric = test_df[non_numeric_columns]\n",
    "    test_numeric = test_df[numeric_columns]\n",
    "    \n",
    "    calib_non_numeric = calib_df[non_numeric_columns]\n",
    "    calib_numeric = calib_df[numeric_columns]\n",
    "\n",
    "    # Handle missing values in numeric columns\n",
    "    train_numeric = train_numeric.dropna()\n",
    "    test_numeric = test_numeric.dropna()\n",
    "    calib_numeric = calib_numeric.dropna()\n",
    "\n",
    "    # Scale the numeric columns\n",
    "    scaler = MinMaxScaler()\n",
    "    train_numeric_scaled = scaler.fit_transform(train_numeric)\n",
    "    test_numeric_scaled = scaler.transform(test_numeric)\n",
    "    calib_numeric_scaled = scaler.transform(calib_numeric)\n",
    "    \n",
    "    # Convert the scaled array back to DataFrames\n",
    "    train_numeric_scaled = pd.DataFrame(train_numeric_scaled, columns=numeric_columns)\n",
    "    test_numeric_scaled = pd.DataFrame(test_numeric_scaled, columns=numeric_columns)\n",
    "    calib_numeric_scaled = pd.DataFrame(calib_numeric_scaled, columns=numeric_columns)\n",
    "    \n",
    "    # Combine the scaled numeric columns with the non-numeric columns\n",
    "    train_scaled = pd.concat([train_numeric_scaled, train_non_numeric.reset_index(drop=True)], axis=1)\n",
    "    test_scaled = pd.concat([test_numeric_scaled, test_non_numeric.reset_index(drop=True)], axis=1)\n",
    "    calib_scaled = pd.concat([calib_numeric_scaled, calib_non_numeric.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return train_scaled, test_scaled, calib_scaled\n",
    "\n",
    "# Example usage\n",
    "# df = pd.read_csv('your_dataset.csv')  # Load your dataset\n",
    "X_train_scaled, X_test_scaled, X_calib_scaled = scale_numeric_columns(X_train, X_test, X_calib )\n",
    "# print(\"Scaled train data:\", X_train_scaled.head())\n",
    "# print(\"Scaled test data:\", X_test_scaled.head())\n",
    "\n",
    "X_train = X_train_scaled.copy()\n",
    "X_test = X_test_scaled.copy()\n",
    "X_calib = X_calib_scaled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b14a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec670bc2",
   "metadata": {},
   "source": [
    "########### Training on XGBoost\n",
    "############\n",
    "############\n",
    "###########\n",
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3513b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe5da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import joblib\n",
    "# import xgboost as xgb\n",
    "# import xgboost as xgb\n",
    "# from mapie.classification import MapieClassifier\n",
    "# from mapie.metrics import classification_coverage_score\n",
    "\n",
    "\n",
    "# # Initialize the XGBoost classifier\n",
    "# xgb_classifier = xgb.XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False)\n",
    "\n",
    "# # Train the classifier\n",
    "# xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# # Apply conformal prediction using the calibration set\n",
    "# mapie_clf = MapieClassifier(xgb_classifier, method=\"score\", cv=\"prefit\")\n",
    "# mapie_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# # Predict with confidence intervals\n",
    "# y_pred, y_pis = mapie_clf.predict(X_test, alpha=0.99)  # 90% prediction intervals\n",
    "\n",
    "\n",
    "\n",
    "# # Predict with prediction sets\n",
    "# y_pred, y_pis = mapie_clf.predict(X_test, alpha=0.5)  # 50% prediction sets to significantly broaden intervals\n",
    "\n",
    "# # Check the structure of y_pis and ensure it contains prediction sets\n",
    "# print(f\"Structure of y_pis: {y_pis.shape}\")\n",
    "\n",
    "# # Diagnose the number of classes included in the prediction sets\n",
    "# num_classes_in_pis = np.sum(y_pis, axis=1)\n",
    "# print(f\"Number of classes in prediction sets: {np.unique(num_classes_in_pis, return_counts=True)}\")\n",
    "\n",
    "# # Check for samples where y_pis has more than one True\n",
    "# samples_with_multiple_classes = [(i, np.where(y_pis[i])[0]) for i in range(len(y_pis)) if np.sum(y_pis[i]) > 1]\n",
    "\n",
    "# # Print the indices and prediction sets of such samples\n",
    "# print(\"Samples with multiple classes in prediction sets:\")\n",
    "# for sample in samples_with_multiple_classes:\n",
    "#     print(f\"Sample index: {sample[0]}, Prediction set: {sample[1]}\")\n",
    "\n",
    "# # Ensure y_pis is transformed correctly for coverage calculation\n",
    "# y_pis_transformed = np.array([set(np.where(y_pis[i])[0]) for i in range(len(y_pis))])\n",
    "\n",
    "# # Create a boolean array for coverage calculation\n",
    "# y_pis_boolean = np.zeros((len(y_test), len(np.unique(y_test))), dtype=bool)\n",
    "# for i in range(len(y_test)):\n",
    "#     y_pis_boolean[i, list(y_pis_transformed[i])] = True\n",
    "\n",
    "# # Calculate coverage\n",
    "# coverage = classification_coverage_score(y_test, y_pis_boolean)\n",
    "# print(f\"Coverage: {coverage:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d016db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Evaluate the classifier\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# report = classification_report(y_test, y_pred)\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "# print('Classification Report:')\n",
    "# print(report)\n",
    "\n",
    "# # Print the mapping between original labels and encoded values\n",
    "# label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "# print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "# # Decode the labels using the dictionary\n",
    "# decoded_labels = [key for key in sorted(label_mapping, key=label_mapping.get)]\n",
    "# print(decoded_labels)\n",
    "\n",
    "# # Plot the confusion matrix\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=decoded_labels, yticklabels=decoded_labels)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.title('Confusion Matrix with Decoded Labels')\n",
    "# plt.show()\n",
    "\n",
    "# # Save the trained model to a file\n",
    "# model_filename = 'xgboost_model.pkl'\n",
    "# joblib.dump(xgb_classifier, model_filename)\n",
    "# print(f'Model saved to {model_filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5dd6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the index of the label 'GLCL'\n",
    "# glcl_index = label_mapping[\"GLCL\"]\n",
    "\n",
    "# # Extract the row and column corresponding to 'GLCL' in the confusion matrix\n",
    "# glcl_conf_matrix_row = conf_matrix[glcl_index, :]\n",
    "# glcl_conf_matrix_col = conf_matrix[:, glcl_index]\n",
    "\n",
    "# # Calculate the number of true positives, false positives, false negatives, and true negatives for 'GLCL'\n",
    "# true_positives_glcl = glcl_conf_matrix_row[glcl_index]\n",
    "# false_positives_glcl = sum(glcl_conf_matrix_col) - true_positives_glcl\n",
    "# false_negatives_glcl = sum(glcl_conf_matrix_row) - true_positives_glcl\n",
    "# true_negatives_glcl = np.sum(conf_matrix) - (true_positives_glcl + false_positives_glcl + false_negatives_glcl)\n",
    "\n",
    "# # Calculate the accuracy for 'GLCL'\n",
    "# accuracy_glcl = (true_positives_glcl + true_negatives_glcl) / np.sum(conf_matrix)\n",
    "\n",
    "# print(f'Number of correct predictions for label GLCL: {true_positives_glcl}')\n",
    "# print(f'Number of false predictions for label GLCL: {false_negatives_glcl + false_positives_glcl}')\n",
    "# print(f'Accuracy for label GLCL: {accuracy_glcl:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef6c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Number of classes\n",
    "# num_classes = conf_matrix.shape[0]\n",
    "\n",
    "# # Initialize lists to store precision, recall, and f1 score for each label\n",
    "# precisions = []\n",
    "# recalls = []\n",
    "# f1_scores = []\n",
    "\n",
    "\n",
    "\n",
    "# # Calculate precision, recall, and f1 score for each class\n",
    "# for i in range(num_classes):\n",
    "#     tp = conf_matrix[i, i]\n",
    "#     fp = np.sum(conf_matrix[:, i]) - tp\n",
    "#     fn = np.sum(conf_matrix[i, :]) - tp\n",
    "\n",
    "#     precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "#     recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "#     f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "#     precisions.append(precision)\n",
    "#     recalls.append(recall)\n",
    "#     f1_scores.append(f1_score)\n",
    "\n",
    "# # Plot Precision, Recall, and F1 Score for each class\n",
    "# labels = list(label_mapping.keys())\n",
    "# x = np.arange(len(labels))\n",
    "# bar_width = 0.3\n",
    "\n",
    "# plt.figure(figsize=(16, 6))\n",
    "\n",
    "# # Plot Precision\n",
    "# plt.subplot(1, 3, 1)\n",
    "# plt.bar(x, precisions, width=bar_width, color='b', alpha=0.7)\n",
    "# plt.xticks(x, labels, rotation=90)\n",
    "# plt.xlabel('Class')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title('Precision for each class')\n",
    "# plt.ylim(0, 1.1)\n",
    "# for i, v in enumerate(precisions):\n",
    "#     plt.text(i, v + 0.02, f'{v:.2f}', ha='center', va='bottom', color='black')\n",
    "\n",
    "# # Plot Recall\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.bar(x, recalls, width=bar_width, color='g', alpha=0.7)\n",
    "# plt.xticks(x, labels, rotation=90)\n",
    "# plt.xlabel('Class')\n",
    "# plt.ylabel('Recall')\n",
    "# plt.title('Recall for each class')\n",
    "# plt.ylim(0, 1.1)\n",
    "# for i, v in enumerate(recalls):\n",
    "#     plt.text(i, v + 0.02, f'{v:.2f}', ha='center', va='bottom', color='black')\n",
    "\n",
    "# # Plot F1 Score\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.bar(x, f1_scores, width=bar_width, color='m', alpha=0.7)\n",
    "# plt.xticks(x, labels, rotation=90)\n",
    "# plt.xlabel('Class')\n",
    "# plt.ylabel('F1 Score')\n",
    "# plt.title('F1 Score for each class')\n",
    "# plt.ylim(0, 1.1)\n",
    "# for i, v in enumerate(f1_scores):\n",
    "#     plt.text(i, v + 0.02, f'{v:.2f}', ha='center', va='bottom', color='black')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a45b3",
   "metadata": {},
   "source": [
    "############## Binary Classification with catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b0a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import catboost as cb\n",
    "import numpy as np\n",
    "from mapie.classification import MapieClassifier\n",
    "from mapie.metrics import classification_coverage_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Decode the encoded labels back to the original labels\n",
    "y_train_decoded = label_encoder.inverse_transform(y_train)\n",
    "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
    "y_calib_decoded = label_encoder.inverse_transform(y_calib)\n",
    "\n",
    "# Important class\n",
    "important_class = \"GLCL\"\n",
    "\n",
    "# Converting to binary labels\n",
    "y_train_binary = pd.Series([1 if label == important_class else 0 for label in y_train_decoded])\n",
    "y_test_binary = pd.Series([1 if label == important_class else 0 for label in y_test_decoded])\n",
    "y_calib_binary = pd.Series([1 if label == important_class else 0 for label in y_calib_decoded])\n",
    "\n",
    "# Create a mapping for visualization\n",
    "binary_to_original_mapping = {\n",
    "    1: important_class,\n",
    "    0: 'Other'\n",
    "}\n",
    "# Define the hyperparameter grid for CatBoost\n",
    "param_grid = {\n",
    "    'iterations': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'depth': [3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bylevel': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the CatBoost classifier\n",
    "catboost_classifier = CatBoostClassifier(random_seed=42, verbose=0)  # Ensure verbose is set to 0 for GridSearchCV\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=catboost_classifier, param_grid=param_grid, \n",
    "                           scoring='f1', cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV model\n",
    "grid_search.fit(X_train, y_train_binary)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best accuracy Score: \", grid_search.best_score_)\n",
    "\n",
    "# Get the best estimator\n",
    "best_catg = grid_search.best_estimator_\n",
    "\n",
    "# Train the XGBoost classifier with the best estimator\n",
    "best_catg.fit(X_train, y_train_binary)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_pred_proba = best_catg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Function to evaluate different thresholds\n",
    "def evaluate_thresholds_for_accuracy(y_test, y_pred_proba, thresholds):\n",
    "    best_threshold = 0.5\n",
    "    best_accuracy = 0.0\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, best_accuracy\n",
    "\n",
    "# Define the range of thresholds to evaluate\n",
    "thresholds = [i * 0.01 for i in range(100)]\n",
    "\n",
    "# Find the best threshold for accuracy\n",
    "best_threshold, best_accuracy = evaluate_thresholds_for_accuracy(y_test_binary, y_pred_proba, thresholds)\n",
    "\n",
    "\n",
    "# Predict using the best threshold\n",
    "y_pred = (y_pred_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the best threshold\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Best accuracy Score:\", best_accuracy)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test_binary, y_pred)\n",
    "report = classification_report(y_test_binary, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test_binary, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Decode the labels using the dictionary\n",
    "decoded_labels = [binary_to_original_mapping[key] for key in sorted(binary_to_original_mapping)]\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=decoded_labels, yticklabels=decoded_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix with Decoded Labels')\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model to a file\n",
    "model_filename = 'catboost_model_Binary_Classification.pkl'\n",
    "joblib.dump(catboost_classifier, model_filename)\n",
    "print(f'Model saved to {model_filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de83bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Mapie for conformal prediction\n",
    "mapie = MapieClassifier(estimator=best_catg, method=\"score\", cv=\"prefit\")\n",
    "\n",
    "# Fit the conformal predictor on training and calibration data combined\n",
    "mapie.fit(X_calib, y_calib_binary)\n",
    "\n",
    "# Get prediction intervals for probabilities\n",
    "prediction, prediction_intervals = mapie.predict(X_test, alpha=0.05)\n",
    "\n",
    "# Inspect the shape of prediction_intervals\n",
    "print(\"Prediction Intervals Shape:\", prediction_intervals.shape)\n",
    "\n",
    "# Extract lower and upper bounds for probabilities\n",
    "lower_bounds = prediction_intervals[:, 0, 0]\n",
    "upper_bounds = prediction_intervals[:, 1, 0]\n",
    "\n",
    "# Calculate the error bars\n",
    "yerr_lower = y_pred_proba - lower_bounds\n",
    "yerr_upper = upper_bounds - y_pred_proba\n",
    "\n",
    "# Ensure yerr contains no negative values\n",
    "yerr_lower = np.maximum(0, yerr_lower)\n",
    "yerr_upper = np.maximum(0, yerr_upper)\n",
    "\n",
    "# Combine probabilities and intervals into a DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Predicted Probability': y_pred_proba,\n",
    "    'Predicted Label': y_pred,\n",
    "    'Lower Bound': lower_bounds,\n",
    "    'Upper Bound': upper_bounds,\n",
    "    'True Label': y_test_binary\n",
    "})\n",
    "\n",
    "# Calculate coverage\n",
    "coverage = classification_coverage_score(y_test_binary, prediction_intervals[:, :, 0])\n",
    "print(\"Coverage:\", coverage)\n",
    "\n",
    "# Print and visualize the results\n",
    "print(results_df.head())\n",
    "\n",
    "# Sample a subset of the data\n",
    "sample_size = 50  # Adjust this to a suitable number for your data\n",
    "sampled_df = results_df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Extract the sampled data\n",
    "sampled_indices = sampled_df.index\n",
    "sampled_predicted_probabilities = sampled_df['Predicted Probability'].values\n",
    "sampled_lower_bounds = sampled_df['Lower Bound'].values\n",
    "sampled_upper_bounds = sampled_df['Upper Bound'].values\n",
    "\n",
    "# Calculate the error bars\n",
    "sampled_yerr_lower = sampled_predicted_probabilities - sampled_lower_bounds\n",
    "sampled_yerr_upper = sampled_upper_bounds - sampled_predicted_probabilities\n",
    "\n",
    "# Ensure yerr contains no negative values\n",
    "sampled_yerr_lower = np.maximum(0, sampled_yerr_lower)\n",
    "sampled_yerr_upper = np.maximum(0, sampled_yerr_upper)\n",
    "\n",
    "# Plot the sampled data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.errorbar(range(len(sampled_predicted_probabilities)), sampled_predicted_probabilities, yerr=[sampled_yerr_lower, sampled_yerr_upper], fmt='o')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Predicted Probability with Intervals')\n",
    "plt.title('Sampled Predicted Probabilities with Conformal Prediction Intervals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7773b036",
   "metadata": {},
   "source": [
    "###########Training on xgboost with binary classification#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "import numpy as np  \n",
    "from mapie.classification import MapieClassifier  \n",
    "from mapie.metrics import classification_coverage_score  \n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Decode the encoded labels back to the original labels\n",
    "y_train_decoded = label_encoder.inverse_transform(y_train)\n",
    "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
    "y_calib_decoded = label_encoder.inverse_transform(y_calib)\n",
    "\n",
    "\n",
    "\n",
    "# Important class\n",
    "important_class = \"GLCL\"\n",
    "\n",
    "# Converting to binary labels\n",
    "y_train_binary = pd.Series([1 if label == important_class else 0 for label in y_train_decoded])\n",
    "y_test_binary = pd.Series([1 if label == important_class else 0 for label in y_test_decoded])\n",
    "y_calib_binary = pd.Series([1 if label == important_class else 0 for label in y_calib_decoded])\n",
    "\n",
    "# Create a mapping for visualization\n",
    "binary_to_original_mapping = {\n",
    "    1: important_class,\n",
    "    0: 'Other'\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, use_label_encoder=False)\n",
    "\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, \n",
    "                           scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV model\n",
    "grid_search.fit(X_train, y_train_binary)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best accuracy Score: \", grid_search.best_score_)\n",
    "\n",
    "# Get the best estimator\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Train the XGBoost classifier with the best estimator\n",
    "best_xgb.fit(X_train, y_train_binary)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_pred_proba = best_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Function to evaluate different thresholds\n",
    "def evaluate_thresholds_for_accuracy(y_test, y_pred_proba, thresholds):\n",
    "    best_threshold = 0.5\n",
    "    best_accuracy = 0.0\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, best_accuracy\n",
    "\n",
    "# Define the range of thresholds to evaluate\n",
    "thresholds = [i * 0.01 for i in range(100)]\n",
    "\n",
    "# Find the best threshold for accuracy\n",
    "best_threshold, best_accuracy = evaluate_thresholds_for_accuracy(y_test_binary, y_pred_proba, thresholds)\n",
    "\n",
    "\n",
    "# Predict using the best threshold\n",
    "y_pred = (y_pred_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the best threshold\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Best accuracy Score:\", best_accuracy)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test_binary, y_pred)  # Changed\n",
    "report = classification_report(y_test_binary, y_pred)  # Changed\n",
    "conf_matrix = confusion_matrix(y_test_binary, y_pred)  # Changed\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Decode the labels using the dictionary\n",
    "decoded_labels = [binary_to_original_mapping[key] for key in sorted(binary_to_original_mapping)]\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=decoded_labels, yticklabels=decoded_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix with Decoded Labels')\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model to a file\n",
    "model_filename = 'xgboost_model_Binary_Classification.pkl'\n",
    "joblib.dump(xgb_classifier, model_filename)\n",
    "print(f'Model saved to {model_filename}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcaa657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Mapie for conformal prediction\n",
    "mapie = MapieClassifier(estimator=best_xgb, method=\"score\", cv=\"prefit\")\n",
    "\n",
    "# Fit the conformal predictor on training and calibration data combined\n",
    "mapie.fit(X_calib, y_calib_binary)\n",
    "\n",
    "# Get prediction intervals for probabilities\n",
    "prediction, prediction_intervals = mapie.predict(X_test, alpha=0.05)\n",
    "\n",
    "# Inspect the shape of prediction_intervals\n",
    "print(\"Prediction Intervals Shape:\", prediction_intervals.shape)\n",
    "\n",
    "# Extract lower and upper bounds for probabilities\n",
    "lower_bounds = prediction_intervals[:, 0, 0]\n",
    "upper_bounds = prediction_intervals[:, 1, 0]\n",
    "\n",
    "# Calculate the error bars\n",
    "yerr_lower = y_pred_proba - lower_bounds\n",
    "yerr_upper = upper_bounds - y_pred_proba\n",
    "\n",
    "# Ensure yerr contains no negative values\n",
    "yerr_lower = np.maximum(0, yerr_lower)\n",
    "yerr_upper = np.maximum(0, yerr_upper)\n",
    "\n",
    "# Combine probabilities and intervals into a DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Predicted Probability': y_pred_proba,\n",
    "    'Predicted Label': y_pred,\n",
    "    'Lower Bound': lower_bounds,\n",
    "    'Upper Bound': upper_bounds,\n",
    "    'True Label': y_test_binary\n",
    "})\n",
    "\n",
    "# Calculate coverage\n",
    "coverage = classification_coverage_score(y_test_binary, prediction_intervals[:, :, 0])\n",
    "print(\"Coverage:\", coverage)\n",
    "\n",
    "# Print and visualize the results\n",
    "print(results_df.head())\n",
    "\n",
    "# Sample a subset of the data\n",
    "sample_size = 50  # Adjust this to a suitable number for your data\n",
    "sampled_df = results_df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Extract the sampled data\n",
    "sampled_indices = sampled_df.index\n",
    "sampled_predicted_probabilities = sampled_df['Predicted Probability'].values\n",
    "sampled_lower_bounds = sampled_df['Lower Bound'].values\n",
    "sampled_upper_bounds = sampled_df['Upper Bound'].values\n",
    "\n",
    "# Calculate the error bars\n",
    "sampled_yerr_lower = sampled_predicted_probabilities - sampled_lower_bounds\n",
    "sampled_yerr_upper = sampled_upper_bounds - sampled_predicted_probabilities\n",
    "\n",
    "# Ensure yerr contains no negative values\n",
    "sampled_yerr_lower = np.maximum(0, sampled_yerr_lower)\n",
    "sampled_yerr_upper = np.maximum(0, sampled_yerr_upper)\n",
    "\n",
    "# Plot the sampled data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.errorbar(range(len(sampled_predicted_probabilities)), sampled_predicted_probabilities, yerr=[sampled_yerr_lower, sampled_yerr_upper], fmt='o')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Predicted Probability with Intervals')\n",
    "plt.title('Sampled Predicted Probabilities with Conformal Prediction Intervals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a5d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store precision, recall, accuracy, and F1 score for each label\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "print(binary_to_original_mapping.items())\n",
    "\n",
    "# Calculate precision, recall, accuracy, and F1 score for each label\n",
    "for index, label in binary_to_original_mapping.items():  # Swap label and index\n",
    "    print(f\"Processing index: {index}, label: {label}\")\n",
    "    \n",
    "    if index == 1:\n",
    "        print(\"Index is 1\")\n",
    "        # True Positives (TP): Diagonal element for the current label\n",
    "        tp = conf_matrix[1, 1]\n",
    "\n",
    "        # False Positives (FP): Sum of the column for the current label excluding the diagonal element\n",
    "        fp = conf_matrix[0, 1]\n",
    "\n",
    "        # False Negatives (FN): Sum of the row for the current label excluding the diagonal element\n",
    "        fn = conf_matrix[1, 0]\n",
    "\n",
    "        # True Negatives (TN): Sum of all elements in the matrix excluding the current row and column\n",
    "        tn = conf_matrix[0, 0]\n",
    "    elif index == 0:\n",
    "        print(\"Index is 0\")\n",
    "        # True Positives (TP): Diagonal element for the current label\n",
    "        tp = conf_matrix[0, 0]\n",
    "\n",
    "        # False Positives (FP): Sum of the column for the current label excluding the diagonal element\n",
    "        fp = conf_matrix[1, 0]\n",
    "\n",
    "        # False Negatives (FN): Sum of the row for the current label excluding the diagonal element\n",
    "        fn = conf_matrix[0, 1]\n",
    "\n",
    "        # True Negatives (TN): Sum of all elements in the matrix excluding the current row and column\n",
    "        tn = conf_matrix[1, 1]\n",
    "    else:\n",
    "        assert False, f\"Unexpected label value: {index}\"  # Assert for unexpected values\n",
    "        print(\"Index is neither 0 nor 1, skipping this index\")\n",
    "        continue  # Skip labels not covered in the binary mapping\n",
    "\n",
    "    # Calculate Precision\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    precisions.append(precision)\n",
    "\n",
    "    # Calculate Recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    recalls.append(recall)\n",
    "\n",
    "    # Calculate F1 Score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    f1_scores.append(f1_score)\n",
    "\n",
    "# Plot Precision, Recall, and F1 Score for each label\n",
    "labels = list(binary_to_original_mapping.values())\n",
    "x = np.arange(len(labels))\n",
    "\n",
    "bar_width = 0.3\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Plot Precision\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(x, precisions, width=bar_width, color='b', alpha=0.7)\n",
    "plt.xticks(x, labels, rotation=90)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision for each class')\n",
    "plt.ylim(0, 1.1)\n",
    "for i, v in enumerate(precisions):\n",
    "    plt.text(i, v + 0.02, f'{v:.2f}', ha='center', va='bottom', color='black')\n",
    "\n",
    "# Plot Recall\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(x, recalls, width=bar_width, color='g', alpha=0.7)\n",
    "plt.xticks(x, labels, rotation=90)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall for each class')\n",
    "plt.ylim(0, 1.1)\n",
    "for i, v in enumerate(recalls):\n",
    "    plt.text(i, v + 0.02, f'{v:.2f}', ha='center', va='bottom', color='black')\n",
    "\n",
    "# Plot F1 Score\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(x, f1_scores, width=bar_width, color='m', alpha=0.7)\n",
    "plt.xticks(x, labels, rotation=90)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score for each class')\n",
    "plt.ylim(0, 1.1)\n",
    "for i, v in enumerate(f1_scores):\n",
    "    plt.text(i, v + 0.02, f'{v:.2f}', ha='center', va='bottom', color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17be4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Scatter plot of (umag - gmag) vs (imag - zmag) with globular clusters highlighted\n",
    "plt.figure(figsize=(10, 8))  # Create a figure with specified size\n",
    "\n",
    "# Plot other sources\n",
    "plt.scatter(\n",
    "    X_test['umag_minus_gmag'],  # Difference between umag and gmag for x-axis\n",
    "    X_test['imag_minus_zmag'],  # Difference between imag and zmag for y-axis\n",
    "    c='gray',  # Color for these points\n",
    "    s=10,\n",
    "    alpha=0.5,  # Transparency level\n",
    "    label='Other Sources'  # Label for legend\n",
    ")\n",
    "\n",
    "# Highlight globular clusters\n",
    "plt.scatter(\n",
    "    X_test['umag_minus_gmag'][y_pred == 1],  # x-axis values for predicted globular clusters\n",
    "    X_test['imag_minus_zmag'][y_pred == 1],  # y-axis values for predicted globular clusters\n",
    "    c='red',  # Color for these points\n",
    "    s=10,\n",
    "    label='Globular Clusters'  # Label for legend\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('umag - gmag')  # Label for x-axis\n",
    "plt.ylabel('imag - zmag')  # Label for y-axis\n",
    "plt.xlim(0, 6)  # Limit the range for x-axis\n",
    "plt.ylim(-1, 2)  # Limit the range for y-axis\n",
    "plt.legend()  # Add legend to the plot\n",
    "plt.title('Scatter Plot of (umag - gmag) vs (imag - zmag)')  # Title of the plot\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlabeled Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a26456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions on the unlabeled dataset\n",
    "# unlabeled_data_proba = xgb_classifier.predict_proba(unlabeled_data)[:, 1]\n",
    "# unlabeled_data_pred = (unlabeled_data_proba >= best_threshold).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e481ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert binary predictions to original labels using the mapping\n",
    "# original_labels = np.array([binary_to_original_mapping[label] for label in unlabeled_data_pred])\n",
    "\n",
    "# # Count occurrences of \"GLCL\" and \"Other\"\n",
    "# glcl_count_un = np.sum(original_labels == \"GLCL\")\n",
    "# other_count_un = np.sum(original_labels == \"Other\")\n",
    "# total_count_un = len(original_labels)\n",
    "\n",
    "# # Calculate percentage of \"GLCL\"\n",
    "# glcl_percentage_un = (glcl_count_un / total_count_un) * 100\n",
    "\n",
    "# print(f\"Number of 'GLCL' labels in Unlabeled dataset: {glcl_count_un}\")\n",
    "# print(f\"Number of 'Other' labels in Unlabeled dataset: {other_count_un}\")\n",
    "# print(f\"Total number of labels in Unlabeled dataset: {total_count_un}\")\n",
    "\n",
    "# print(f\"Percentage of 'GLCL' labels in Unlabeled dataset: {glcl_percentage_un:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52503440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot other sources\n",
    "# plt.scatter(\n",
    "#     unlabeled_data['umag_minus_gmag'],  # Difference between umag and gmag for x-axis\n",
    "#     unlabeled_data['imag_minus_zmag'],  # Difference between imag and zmag for y-axis\n",
    "#     c='gray',  # Color for these points\n",
    "#     s=10,\n",
    "#     alpha=0.5,  # Transparency level\n",
    "#     label='Other Sources'  # Label for legend\n",
    "# )\n",
    "\n",
    "# # Highlight predicted globular clusters\n",
    "# plt.scatter(\n",
    "#     unlabeled_data['umag_minus_gmag'][unlabeled_data_pred == 1],  # x-axis values for predicted globular clusters\n",
    "#     unlabeled_data['imag_minus_zmag'][unlabeled_data_pred == 1],  # y-axis values for predicted globular clusters\n",
    "#     c='red',  # Color for these points\n",
    "#     s=20,\n",
    "#     label='Predicted Globular Clusters'  # Label for legend\n",
    "# )\n",
    "\n",
    "# # Add labels and title\n",
    "# plt.xlabel('umag - gmag')  # Label for x-axis\n",
    "# plt.ylabel('imag - zmag')  # Label for y-axis\n",
    "# plt.xlim(0, 6)  # Limit the range for x-axis\n",
    "# plt.ylim(-1, 2)  # Limit the range for y-axis\n",
    "# plt.legend()  # Add legend to the plot\n",
    "# plt.title('Scatter Plot of Unlabeled Data (umag - gmag) vs (imag - zmag)')  # Title of the plot\n",
    "\n",
    "# # Display the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ccdc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.manifold import TSNE\n",
    "# import umap.umap_ as umap\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# # Select the relevant columns for dimensionality reduction\n",
    "# # features = ['umag', 'gmag', 'rmag', 'imag', 'zmag', ]\n",
    "# features = ['umag_minus_gmag', 'umag_minus_rmag', 'umag_minus_imag',\n",
    "#        'umag_minus_zmag', 'gmag_minus_rmag', 'gmag_minus_imag',\n",
    "#        'gmag_minus_zmag', 'rmag_minus_imag', 'rmag_minus_zmag',\n",
    "#        'imag_minus_zmag']\n",
    "\n",
    "# unlabeled_data_features = unlabeled_data[features]\n",
    "\n",
    "# # Standardize the data\n",
    "# tsne_scaler = StandardScaler()\n",
    "# unlabeled_data_scaled = tsne_scaler.fit_transform(unlabeled_data_features)\n",
    "\n",
    "# # Apply t-SNE\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# tsne_results = tsne.fit_transform(unlabeled_data_scaled)\n",
    "\n",
    "# # Create a DataFrame for the t-SNE results\n",
    "# tsne_df = pd.DataFrame(tsne_results, columns=['TSNE1', 'TSNE2'],  index=unlabeled_data.index)\n",
    "\n",
    "# # Apply UMAP\n",
    "# umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "# umap_results = umap_model.fit_transform(unlabeled_data_scaled)\n",
    "\n",
    "# # Create a DataFrame for the UMAP results\n",
    "# umap_df = pd.DataFrame(umap_results, columns=['UMAP1', 'UMAP2'],  index=unlabeled_data.index)\n",
    "\n",
    "\n",
    "# # Visualize t-SNE results\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.scatter(tsne_df['TSNE1'], tsne_df['TSNE2'], c='gray', s=10, alpha=0.5, label='Other Sources')\n",
    "# plt.scatter(tsne_df['TSNE1'][unlabeled_data_pred == 1], tsne_df['TSNE2'][unlabeled_data_pred == 1], c='red', s=10, label='Predicted Globular Clusters')\n",
    "# plt.xlabel('TSNE1')\n",
    "# plt.ylabel('TSNE2')\n",
    "# plt.legend()\n",
    "# plt.title('t-SNE Scatter Plot of Unlabeled Data')\n",
    "# plt.show()\n",
    "\n",
    "# # Visualize UMAP results\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.scatter(umap_df['UMAP1'], umap_df['UMAP2'], c='gray', s=10, alpha=0.5, label='Other Sources')\n",
    "# plt.scatter(umap_df['UMAP1'][unlabeled_data_pred == 1], umap_df['UMAP2'][unlabeled_data_pred == 1], c='red', s=10, label='Predicted Globular Clusters')\n",
    "# plt.xlabel('UMAP1')\n",
    "# plt.ylabel('UMAP2')\n",
    "# plt.legend()\n",
    "# plt.title('UMAP Scatter Plot of Unlabeled Data')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a46ff5",
   "metadata": {},
   "source": [
    "Countdown the targets predicted on unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a51e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique values of the predicted labels\n",
    "# unique_predictions = pd.Series(unlabeled_predictions_decoded).unique()\n",
    "# print(f'Unique predicted labels: {unique_predictions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b988b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "########### Training on Random Forest\n",
    "############\n",
    "############\n",
    "###########\n",
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a04c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import joblib\n",
    "# # Initialize the Random Forest classifier\n",
    "# rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Train the classifier\n",
    "# rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# # Evaluate the classifier\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# report = classification_report(y_test, y_pred)\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "# print('Classification Report:')\n",
    "# print(report)\n",
    "\n",
    "# # Print the mapping between original labels and encoded values\n",
    "# label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "# print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "# # Decode the labels using the dictionary\n",
    "# decoded_labels = [key for key in sorted(label_mapping, key=label_mapping.get)]\n",
    "# print(decoded_labels)\n",
    "\n",
    "# # Plot the confusion matrix\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=decoded_labels, yticklabels=decoded_labels)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.title('Confusion Matrix with Decoded Labels')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # Feature importance\n",
    "# feature_importances = rf_classifier.feature_importances_\n",
    "# features = X_train.columns\n",
    "\n",
    "# # Plot feature importances\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.barplot(x=feature_importances, y=features)\n",
    "# plt.xlabel('Feature Importance')\n",
    "# plt.ylabel('Features')\n",
    "# plt.title('Feature Importances in Random Forest Classifier')\n",
    "# plt.show()\n",
    "\n",
    "# # Save the trained model to a file\n",
    "# model_filename = 'random_forest_model.pkl'\n",
    "# joblib.dump(rf_classifier, model_filename)\n",
    "# print(f'Model saved to {model_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c955c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Number of classes\n",
    "# num_classes = conf_matrix.shape[0]\n",
    "\n",
    "# # Initialize lists to store precision, recall, and f1 score for each label\n",
    "# precisions = []\n",
    "# recalls = []\n",
    "# f1_scores = []\n",
    "\n",
    "\n",
    "\n",
    "# # Calculate precision, recall, and f1 score for each class\n",
    "# for i in range(num_classes):\n",
    "#     tp = conf_matrix[i, i]\n",
    "#     fp = np.sum(conf_matrix[:, i]) - tp\n",
    "#     fn = np.sum(conf_matrix[i, :]) - tp\n",
    "\n",
    "#     precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "#     recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "#     f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "#     precisions.append(precision)\n",
    "#     recalls.append(recall)\n",
    "#     f1_scores.append(f1_score)\n",
    "\n",
    "# # Plot Precision, Recall, and F1 Score for each class\n",
    "# labels = list(label_mapping.keys())\n",
    "# x = np.arange(len(labels))\n",
    "# bar_width = 0.3\n",
    "\n",
    "# plt.figure(figsize=(16, 6))\n",
    "\n",
    "# # Plot Precision\n",
    "# plt.subplot(1, 3, 1)\n",
    "# plt.bar(x, precisions, width=bar_width, color='b', alpha=0.7)\n",
    "# plt.xticks(x, labels, rotation=90)\n",
    "# plt.xlabel('Class')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title('Precision for each class')\n",
    "# plt.ylim(0, 1.1)\n",
    "# for i, v in enumerate(precisions):\n",
    "#     plt.text(i, v + 0.02, f'{v:.2f}', ha='center', va='bottom', color='black')\n",
    "\n",
    "# # Plot Recall\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.bar(x, recalls, width=bar_width, color='g', alpha=0.7)\n",
    "# plt.xticks(x, labels, rotation=90)\n",
    "# plt.xlabel('Class')\n",
    "# plt.ylabel('Recall')\n",
    "# plt.title('Recall for each class')\n",
    "# plt.ylim(0, 1.1)\n",
    "# for i, v in enumerate(recalls):\n",
    "#     plt.text(i, v + 0.02, f'{v:.2f}', ha='center', va='bottom', color='black')\n",
    "\n",
    "# # Plot F1 Score\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.bar(x, f1_scores, width=bar_width, color='m', alpha=0.7)\n",
    "# plt.xticks(x, labels, rotation=90)\n",
    "# plt.xlabel('Class')\n",
    "# plt.ylabel('F1 Score')\n",
    "# plt.title('F1 Score for each class')\n",
    "# plt.ylim(0, 1.1)\n",
    "# for i, v in enumerate(f1_scores):\n",
    "#     plt.text(i, v + 0.02, f'{v:.2f}', ha='center', va='bottom', color='black')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3911f472",
   "metadata": {},
   "source": [
    "######## Training on random Forest by Binarization #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c60161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import joblib\n",
    "\n",
    "# # Decode the encoded labels back to the original labels\n",
    "# y_train_decoded = label_encoder.inverse_transform(y_train)\n",
    "# y_test_decoded = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "# # Important class\n",
    "# important_class = \"GLCL\"\n",
    "\n",
    "# # Converting to binary labels\n",
    "# y_train_binary = pd.Series([1 if label == important_class else 0 for label in y_train_decoded])\n",
    "# y_test_binary = pd.Series([1 if label == important_class else 0 for label in y_test_decoded])\n",
    "\n",
    "# # Create a mapping for visualization\n",
    "# binary_to_original_mapping = {\n",
    "#     1: important_class,\n",
    "#     0: 'Other'\n",
    "# }\n",
    "\n",
    "# # Initialize the Random Forest classifier\n",
    "# rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Train the classifier\n",
    "# rf_classifier.fit(X_train, y_train_binary)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# # Evaluate the classifier\n",
    "# accuracy = accuracy_score(y_test_binary, y_pred)\n",
    "# report = classification_report(y_test_binary, y_pred)\n",
    "# conf_matrix = confusion_matrix(y_test_binary, y_pred)\n",
    "\n",
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "# print('Classification Report:')\n",
    "# print(report)\n",
    "\n",
    "# # Decode the labels using the dictionary\n",
    "# decoded_labels = [binary_to_original_mapping[key] for key in sorted(binary_to_original_mapping)]\n",
    "\n",
    "# # Plot the confusion matrix\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=decoded_labels, yticklabels=decoded_labels)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.title('Confusion Matrix with Decoded Labels')\n",
    "# plt.show()\n",
    "\n",
    "# # Save the trained model to a file\n",
    "# model_filename = 'RandomForest_model_Binary_Classification.pkl'\n",
    "# joblib.dump(rf_classifier, model_filename)\n",
    "# print(f'Model saved to {model_filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1703b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Initialize lists to store precision, recall, accuracy, and F1 score for each label\n",
    "# precisions = []\n",
    "# recalls = []\n",
    "# f1_scores = []\n",
    "\n",
    "# print(binary_to_original_mapping.items())\n",
    "\n",
    "# # Calculate precision, recall, accuracy, and F1 score for each label\n",
    "# for index, label in binary_to_original_mapping.items():  # Swap label and index\n",
    "#     print(f\"Processing index: {index}, label: {label}\")\n",
    "    \n",
    "#     if index == 1:\n",
    "#         print(\"Index is 1\")\n",
    "#         # True Positives (TP): Diagonal element for the current label\n",
    "#         tp = conf_matrix[1, 1]\n",
    "\n",
    "#         # False Positives (FP): Sum of the column for the current label excluding the diagonal element\n",
    "#         fp = conf_matrix[0, 1]\n",
    "\n",
    "#         # False Negatives (FN): Sum of the row for the current label excluding the diagonal element\n",
    "#         fn = conf_matrix[1, 0]\n",
    "\n",
    "#         # True Negatives (TN): Sum of all elements in the matrix excluding the current row and column\n",
    "#         tn = conf_matrix[0, 0]\n",
    "#     elif index == 0:\n",
    "#         print(\"Index is 0\")\n",
    "#         # True Positives (TP): Diagonal element for the current label\n",
    "#         tp = conf_matrix[0, 0]\n",
    "\n",
    "#         # False Positives (FP): Sum of the column for the current label excluding the diagonal element\n",
    "#         fp = conf_matrix[1, 0]\n",
    "\n",
    "#         # False Negatives (FN): Sum of the row for the current label excluding the diagonal element\n",
    "#         fn = conf_matrix[0, 1]\n",
    "\n",
    "#         # True Negatives (TN): Sum of all elements in the matrix excluding the current row and column\n",
    "#         tn = conf_matrix[1, 1]\n",
    "#     else:\n",
    "#         assert False, f\"Unexpected label value: {index}\"  # Assert for unexpected values\n",
    "#         print(\"Index is neither 0 nor 1, skipping this index\")\n",
    "#         continue  # Skip labels not covered in the binary mapping\n",
    "\n",
    "#     # Calculate Precision\n",
    "#     precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "#     precisions.append(precision)\n",
    "\n",
    "#     # Calculate Recall\n",
    "#     recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "#     recalls.append(recall)\n",
    "\n",
    "#     # Calculate F1 Score\n",
    "#     f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "#     f1_scores.append(f1_score)\n",
    "\n",
    "# # Plot Precision, Recall, and F1 Score for each label\n",
    "# labels = list(binary_to_original_mapping.values())\n",
    "# x = np.arange(len(labels))\n",
    "\n",
    "# bar_width = 0.3\n",
    "\n",
    "# plt.figure(figsize=(16, 6))\n",
    "\n",
    "# # Plot Precision\n",
    "# plt.subplot(1, 3, 1)\n",
    "# plt.bar(x, precisions, width=bar_width, color='b', alpha=0.7)\n",
    "# plt.xticks(x, labels, rotation=90)\n",
    "# plt.xlabel('Class')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title('Precision for each class')\n",
    "# plt.ylim(0, 1.1)\n",
    "# for i, v in enumerate(precisions):\n",
    "#     plt.text(i, v + 0.02, f'{v:.2f}', ha='center', va='bottom', color='black')\n",
    "\n",
    "# # Plot Recall\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.bar(x, recalls, width=bar_width, color='g', alpha=0.7)\n",
    "# plt.xticks(x, labels, rotation=90)\n",
    "# plt.xlabel('Class')\n",
    "# plt.ylabel('Recall')\n",
    "# plt.title('Recall for each class')\n",
    "# plt.ylim(0, 1.1)\n",
    "# for i, v in enumerate(recalls):\n",
    "#     plt.text(i, v + 0.02, f'{v:.2f}', ha='center', va='bottom', color='black')\n",
    "\n",
    "# # Plot F1 Score\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.bar(x, f1_scores, width=bar_width, color='m', alpha=0.7)\n",
    "# plt.xticks(x, labels, rotation=90)\n",
    "# plt.xlabel('Class')\n",
    "# plt.ylabel('F1 Score')\n",
    "# plt.title('F1 Score for each class')\n",
    "# plt.ylim(0, 1.1)\n",
    "# for i, v in enumerate(f1_scores):\n",
    "#     plt.text(i, v + 0.02, f'{v:.2f}', ha='center', va='bottom', color='black')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
